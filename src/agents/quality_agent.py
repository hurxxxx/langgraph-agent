"""
Quality Measurement Agent for Multi-Agent System

This module implements a quality measurement agent that evaluates the quality of responses
generated by other agents. It assesses various aspects such as:
- Accuracy
- Relevance
- Completeness
- Clarity
- Helpfulness

The agent provides scores and feedback for improving response quality.
"""

import os
from typing import Dict, List, Any, Optional, TypedDict, Annotated, Literal, Union
from pydantic import BaseModel, Field

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder


class QualityCriterion(BaseModel):
    """Model for a quality criterion."""
    name: str
    description: str
    score: Optional[float] = None
    feedback: Optional[str] = None


class QualityEvaluation(BaseModel):
    """Model for a quality evaluation result."""
    criteria: List[QualityCriterion]
    overall_score: float
    overall_feedback: str


class QualityAgentConfig(BaseModel):
    """Configuration for the quality measurement agent."""
    llm_model: str = "gpt-4o"
    temperature: float = 0
    streaming: bool = True
    criteria: List[QualityCriterion] = [
        QualityCriterion(
            name="accuracy",
            description="Is the information accurate and factual?"
        ),
        QualityCriterion(
            name="relevance",
            description="Is the response relevant to the query?"
        ),
        QualityCriterion(
            name="completeness",
            description="Does the response fully address all aspects of the query?"
        ),
        QualityCriterion(
            name="clarity",
            description="Is the response clear and easy to understand?"
        ),
        QualityCriterion(
            name="helpfulness",
            description="Is the response helpful for the user's needs?"
        )
    ]
    system_message: str = """
    You are a quality measurement agent that evaluates the quality of responses.
    Your job is to:
    1. Understand the original query
    2. Evaluate the response based on the specified criteria
    3. Provide scores and feedback for each criterion
    4. Calculate an overall score and provide overall feedback

    Be objective and thorough in your evaluation.
    """


class QualityAgent:
    """
    Quality measurement agent that evaluates response quality.
    """

    def __init__(self, config: QualityAgentConfig = QualityAgentConfig()):
        """
        Initialize the quality measurement agent.

        Args:
            config: Configuration for the quality agent
        """
        self.config = config

        # Initialize LLM
        try:
            self.llm = ChatOpenAI(
                model=config.llm_model,
                temperature=config.temperature,
                streaming=config.streaming
            )
        except Exception as e:
            print(f"Warning: Could not initialize ChatOpenAI: {str(e)}")
            # Use a mock implementation
            class MockLLM:
                def invoke(self, messages):
                    return {"content": """
                    ## Criteria Evaluation

                    ### accuracy
                    Score: 0.8
                    Feedback: The response is generally accurate.

                    ### clarity
                    Score: 0.9
                    Feedback: The response is clear and well-structured.

                    ### helpfulness
                    Score: 0.85
                    Feedback: The response is helpful for the user's needs.

                    ## Overall Evaluation

                    Overall Score: 0.85
                    Overall Feedback: This is a good response that meets the user's needs.
                    """}
            self.llm = MockLLM()

        # Create evaluation prompt template
        criteria_text = "\n".join([
            f"- {criterion.name}: {criterion.description}"
            for criterion in config.criteria
        ])

        self.evaluation_prompt = ChatPromptTemplate.from_messages([
            SystemMessage(content=f"{config.system_message}\n\nEvaluation criteria:\n{criteria_text}"),
            SystemMessage(content="Original query: {query}"),
            SystemMessage(content="Response to evaluate: {response}"),
            HumanMessage(content="""
            Please evaluate the response based on the criteria.
            For each criterion, provide:
            1. A score from 0.0 to 1.0 (where 1.0 is perfect)
            2. Specific feedback explaining the score

            Then provide an overall score (average of all criteria) and overall feedback.

            Format your response as follows:

            ## Criteria Evaluation

            ### [Criterion Name]
            Score: [Score]
            Feedback: [Feedback]

            ### [Criterion Name]
            Score: [Score]
            Feedback: [Feedback]

            ...

            ## Overall Evaluation

            Overall Score: [Overall Score]
            Overall Feedback: [Overall Feedback]
            """)
        ])

    def _parse_evaluation_response(self, response: str) -> QualityEvaluation:
        """
        Parse the evaluation response from the LLM.

        Args:
            response: LLM response text

        Returns:
            QualityEvaluation: Parsed evaluation
        """
        # This is a simplified implementation
        # In a real system, you would use more robust parsing

        criteria = []
        overall_score = 0.0
        overall_feedback = ""

        # Parse criteria evaluations
        for criterion in self.config.criteria:
            # Look for the criterion section
            criterion_header = f"### {criterion.name}"
            if criterion_header.lower() in response.lower():
                # Extract the section
                start_idx = response.lower().find(criterion_header.lower())
                end_idx = response.lower().find("###", start_idx + 1)
                if end_idx == -1:
                    end_idx = response.lower().find("## overall", start_idx + 1)
                if end_idx == -1:
                    end_idx = len(response)

                section = response[start_idx:end_idx]

                # Extract score
                score = 0.0
                if "score:" in section.lower():
                    score_line = section.lower().split("score:")[1].split("\n")[0].strip()
                    try:
                        score = float(score_line)
                    except ValueError:
                        # Try to extract the number from text
                        import re
                        numbers = re.findall(r"[-+]?\d*\.\d+|\d+", score_line)
                        if numbers:
                            score = float(numbers[0])

                # Extract feedback
                feedback = ""
                if "feedback:" in section.lower():
                    feedback = section.lower().split("feedback:")[1].strip()
                    # Clean up the feedback
                    if "###" in feedback:
                        feedback = feedback.split("###")[0].strip()
                    if "##" in feedback:
                        feedback = feedback.split("##")[0].strip()

                criteria.append(QualityCriterion(
                    name=criterion.name,
                    description=criterion.description,
                    score=score,
                    feedback=feedback
                ))

        # Parse overall evaluation
        if "## overall" in response.lower():
            overall_section = response.lower().split("## overall")[1]

            # Extract overall score
            if "overall score:" in overall_section:
                score_line = overall_section.split("overall score:")[1].split("\n")[0].strip()
                try:
                    overall_score = float(score_line)
                except ValueError:
                    # Try to extract the number from text
                    import re
                    numbers = re.findall(r"[-+]?\d*\.\d+|\d+", score_line)
                    if numbers:
                        overall_score = float(numbers[0])

            # Extract overall feedback
            if "overall feedback:" in overall_section:
                overall_feedback = overall_section.split("overall feedback:")[1].strip()

        # If no overall score was found, calculate the average
        if overall_score == 0.0 and criteria:
            scores = [c.score for c in criteria if c.score is not None]
            if scores:
                overall_score = sum(scores) / len(scores)

        return QualityEvaluation(
            criteria=criteria,
            overall_score=overall_score,
            overall_feedback=overall_feedback
        )

    def evaluate_response(self, query: str, response: str) -> QualityEvaluation:
        """
        Evaluate the quality of a response.

        Args:
            query: Original user query
            response: Response to evaluate

        Returns:
            QualityEvaluation: Evaluation results
        """
        # Generate evaluation using LLM
        evaluation_response = self.llm.invoke(
            self.evaluation_prompt.format(
                query=query,
                response=response
            )
        )

        # Parse the evaluation response
        return self._parse_evaluation_response(evaluation_response.content)

    def __call__(self, state: Dict[str, Any]) -> Dict[str, Any]:
        """
        Process a state update in the multi-agent system.

        Args:
            state: Current state of the system

        Returns:
            Dict[str, Any]: Updated state
        """
        # Extract the original query and response to evaluate
        messages = state["messages"]

        # Find the last user query
        user_queries = [msg for msg in messages if msg["role"] == "user"]
        if not user_queries:
            # No user query found
            state["agent_outputs"]["quality"] = {
                "error": "No user query found to evaluate"
            }
            state["messages"].append({
                "role": "assistant",
                "content": "I couldn't find a user query to evaluate."
            })
            return state

        query = user_queries[-1]["content"]

        # Find the last assistant response
        assistant_responses = [msg for msg in messages if msg["role"] == "assistant"]
        if not assistant_responses:
            # No assistant response found
            state["agent_outputs"]["quality"] = {
                "error": "No assistant response found to evaluate"
            }
            state["messages"].append({
                "role": "assistant",
                "content": "I couldn't find an assistant response to evaluate."
            })
            return state

        response = assistant_responses[-1]["content"]

        # Evaluate the response
        evaluation = self.evaluate_response(query, response)

        # Format the evaluation for display
        formatted_evaluation = f"""
        # Quality Evaluation

        ## Criteria Scores

        {chr(10).join([
            f"### {criterion.name}\nScore: {criterion.score:.2f}\nFeedback: {criterion.feedback}"
            for criterion in evaluation.criteria
        ])}

        ## Overall Evaluation

        Overall Score: {evaluation.overall_score:.2f}
        Overall Feedback: {evaluation.overall_feedback}
        """

        # Update state
        state["agent_outputs"]["quality"] = evaluation.model_dump()
        state["messages"].append({
            "role": "assistant",
            "content": formatted_evaluation
        })

        return state


# Example usage
if __name__ == "__main__":
    # Create quality agent
    quality_agent = QualityAgent()

    # Test with a query and response
    state = {
        "messages": [
            {"role": "user", "content": "What are the main causes of climate change?"},
            {"role": "assistant", "content": "Climate change is primarily caused by human activities, especially the burning of fossil fuels like coal, oil, and natural gas, which releases greenhouse gases into the atmosphere. Deforestation also contributes by reducing the number of trees that can absorb carbon dioxide."}
        ],
        "agent_outputs": {}
    }

    updated_state = quality_agent(state)
    print(updated_state["messages"][-1]["content"])
